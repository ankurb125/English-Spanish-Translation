# -*- coding: utf-8 -*-
"""mt5_new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10jEycYphfQ-I_FJygVrPANevic9_XV1o
"""

!pip install datasets
!pip install transformers sentencepiece datasets

import pandas as pd
import numpy as np
import matplotlib as plt
from sklearn.model_selection import train_test_split
import torch
import multiprocessing
from itertools import chain
from datasets import Dataset
import datasets
from datasets import load_dataset
from datasets.dataset_dict import DatasetDict

# with open('../datasets/NLP_Data/train.quy.txt','r') as f:
with open('train.quy.txt','r') as f:
    quy_lines = f.readlines()
X = list(quy_lines)
len(X)

# with open('../datasets/NLP_Data/train.es.txt','r') as f:
with open('train.es.txt','r') as f:
    es_lines = f.readlines()
y =list(es_lines)
len(y)

# with open('../datasets/NLP_Data/dict.es.txt') as f:
with open('dict.es.txt') as f:
  es_words =f.readlines()
# with open('../datasets/NLP_Data/dict.quy.txt') as f:
with open('dict.quy.txt') as f:
  quy_words=f.readlines()
es_vocab=list(es_words)
quy_vocab=list(quy_words)

# X[1000:1100]

special_characters=['\n','Â©']
quy_sentences=[]
es_sentences=[]
for row in range(len(X)):
  X[row] = ''.join([i for i in X[row] if not i.isdigit()])
  words = X[row].split()
  X[row]=X[row].lower()
  quy_sentence = ''.join([w for w in X[row] if w not in special_characters])

  quy_sentences.append(quy_sentence)
for row in range(len(y)):
  y[row] = ''.join([i for i in y[row] if not i.isdigit()])
  words = y[row].split()
  y[row]=y[row].lower()
  es_sentence = ''.join([w for w in y[row] if w not in special_characters])
  es_sentences.append(es_sentence)

# quy_sentences[1000:1100]

df=pd.DataFrame(list(zip(quy_sentences,es_sentences)),columns=['quy','es'])
df.to_csv('data.csv',index=False)

train_df,test_df=train_test_split(df,test_size=0.1,random_state=0)
test_df, validation_df = train_test_split(test_df,test_size=0.5)
train_dataset = Dataset.from_dict(train_df)
validation_dataset = Dataset.from_dict(validation_df)
test_dataset = Dataset.from_dict(test_df)
my_dataset = datasets.DatasetDict({"train":train_dataset,'validation':validation_dataset,"test":test_dataset})

my_dataset

my_dataset['train'][10:20]

import transformers
from transformers import MT5Model, T5Tokenizer, DataCollatorWithPadding, AutoTokenizer,AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer
model = AutoModelForSeq2SeqLM.from_pretrained("google/mt5-small")
tokenizer = AutoTokenizer.from_pretrained("google/mt5-small", return_tensors="pt")

quy_sentence = my_dataset["train"][5]["quy"]
es_sentence = my_dataset["train"][5]["es"]
inputs = tokenizer(quy_sentence, text_target=es_sentence)
inputs

wrong_targets = tokenizer(es_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(inputs["labels"]))

max_length=128
prefix = ""
max_input_length = max_length
max_target_length = max_length
def preprocess_function(examples):
    inputs = [prefix + ex for ex in examples["quy"]]
    targets = [ex for ex in examples["es"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)
    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_datasets = my_dataset.map(preprocess_function, batched=True)

preprocess_function(my_dataset['train'][:5])

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# !pip install sacrebleu

from datasets import load_metric

metric = load_metric("sacrebleu")

predictions = [
    "I am trying my level best to make this script run."
]
references = [
    [
        "I am trying several times to make this notebook work."
    ]
]
print(metric.compute(predictions=predictions, references=references))

def compute_metrics(eval_preds):
    preds, labels = eval_preds

    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    print({'bleu': result['score']})
    return {'bleu': result['score']}

batch_size = 16
model_checkpoint =" google/mt5-small"
model_name = model_checkpoint.split("/")[-1]
source_lang = "quy"
target_lang = "es"
args = Seq2SeqTrainingArguments(
    f"{model_name}-finetuned-{source_lang}-to-{target_lang}",
      evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True

)





# from huggingface_hub import notebook_login
# notebook_login()
# torch.cuda.empty_cache()

from transformers import Trainer
trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.evaluate(max_length=max_length)

trainer.train()

trainer.evaluate(max_length=max_length)
# trainer.evaluate(
#             max_length=max_length,  metric_key_prefix="eval"
#         )